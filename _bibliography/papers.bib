
@article{zhou_what_2023,
	title = {What affects {Priming} {Strength}? {Simulating} {Structural} {Priming} {Effect} with {PIPS}},
	volume = {6},
	shorttitle = {What affects {Priming} {Strength}?},
	url = {https://par.nsf.gov/biblio/10439878-what-affects-priming-strength-simulating-structural-priming-effect-pips},
	language = {en},
	urldate = {2024-09-02},
	journal = {Proceedings of the Society for Computation in Linguistics},
	author = {Zhou, Zhenghao and Frank, Robert},
	month = jan,
	year = {2023},
	file = {Full Text PDF:/Users/herbertzhou/Zotero/storage/3RDZ3QEK/Zhou and Frank - 2023 - What affects Priming Strength Simulating Structural Priming Effect with PIPS.pdf:application/pdf},
}

@misc{zhou_is_2024,
	title = {Is {In}-{Context} {Learning} a {Type} of {Gradient}-{Based} {Learning}? {Evidence} from the {Inverse} {Frequency} {Effect} in {Structural} {Priming}},
	shorttitle = {Is {In}-{Context} {Learning} a {Type} of {Gradient}-{Based} {Learning}?},
	url = {http://arxiv.org/abs/2406.18501},
	doi = {10.48550/arXiv.2406.18501},
	abstract = {Large language models (LLMs) have shown the emergent capability of in-context learning (ICL). One line of research has explained ICL as functionally performing gradient descent. In this paper, we introduce a new way of diagnosing whether ICL is functionally equivalent to gradient-based learning. Our approach is based on the inverse frequency effect (IFE) -- a phenomenon in which an error-driven learner is expected to show larger updates when trained on infrequent examples than frequent ones. The IFE has previously been studied in psycholinguistics because humans show this effect in the context of structural priming (the tendency for people to produce sentence structures they have encountered recently); the IFE has been used as evidence that human structural priming must involve error-driven learning mechanisms. In our experiments, we simulated structural priming within ICL and found that LLMs display the IFE, with the effect being stronger in larger models. We conclude that ICL is indeed a type of gradient-based learning, supporting the hypothesis that a gradient component is implicitly computed in the forward pass during ICL. Our results suggest that both humans and LLMs make use of gradient-based, error-driven processing mechanisms.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Zhou, Zhenghao and Frank, Robert and McCoy, R. Thomas},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18501 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/herbertzhou/Zotero/storage/WCEVETQ8/Zhou et al. - 2024 - Is In-Context Learning a Type of Gradient-Based Learning Evidence from the Inverse Frequency Effect.pdf:application/pdf;arXiv.org Snapshot:/Users/herbertzhou/Zotero/storage/7PYJWVR6/2406.html:text/html},
}

@inproceedings{wilson_subject-verb_2023,
	address = {Amherst, MA},
	title = {Subject-verb agreement with {Seq2Seq} transformers: {Bigger} is better, but still not best},
	shorttitle = {Subject-verb agreement with {Seq2Seq} transformers},
	url = {https://aclanthology.org/2023.scil-1.24},
	urldate = {2024-09-02},
	booktitle = {Proceedings of the {Society} for {Computation} in {Linguistics} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Wilson, Michael and Zhou, Zhenghao and Frank, Robert},
	editor = {Hunter, Tim and Prickett, Brandon},
	month = jun,
	year = {2023},
	pages = {278--288},
	file = {Full Text PDF:/Users/herbertzhou/Zotero/storage/W5WZGLMC/Wilson et al. - 2023 - Subject-verb agreement with Seq2Seq transformers Bigger is better, but still not best.pdf:application/pdf},
}
